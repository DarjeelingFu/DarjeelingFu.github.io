---
layout: post
title: "NCP and word2vec"
date: 2020-11-17 21:23:04 +0800
summary: word2vec词向量与自然语言处理
outerCover: 
innerCover: 
categories: 
tags: [机器学习]
---

## 自然语言处理 Natural Language Processing

自然语言处理是指让计算机处理人类能够理解的自然语言并达成一定目的，比如智能问答，名词实体识别等等。

对机器学习而言，其学习模型无非可以用```f(x) = y```表示，即，给定一系列输入，通过模型内部的复杂运算，最终得到结果。在自然语言处理领域，处理的基本对象为词语，因而```x```可以是一个或多个词语，其输出```y```则可以是```x```的词性，或者```x```的同义词等等。

机器学习的模型仅接受数字化的输入，因而要想将自然语言作为输入，就必须找到方法把语言文字转换为数字数据，该过程被称为词嵌入（嵌入到数字空间）。word2vec是常见的词嵌入方法。

## Word2vec词嵌入方法

word2vec的愿景是用一个定长的n-dim向量唯一表示一个词语，该向量的各维往往有自己独特的意义（比如不同维度代表不同情感色彩的强弱，（解释不固定）），本质上是用语言模型（Language Model）来表示词语。需要注意的是，word2vec的获得本身就是机器学习的结果，下面介绍其具体获得过程。

既然word2vec词向量可以通过机器学习获得，那么可以从训练集，隐藏层设置，数据输出三个层面来理解其获得过程。

首先，word2vec学习模型的输入可以一个词语，该词语用其```one-hot```编码表示，也即，设整个词表中有```N```个词语，并按一定次序排列，那么第```n```个词语的```one-hot```编码为一个N维向量，其中除第```n```位为```1```外，其他维都为```0```。这样一来，词表中的词语就可以被唯一标定。到目前为止，好像已经找到了用向量唯一表示词语的方法，似乎已经没有必要再使用word2vec获得其词向量。的确，one-hot编码可以作为词语的向量表示，但是它过于稀疏，会造成存储空间的浪费，同时这种向量表示仅仅是简单的标记，缺少具体含义，这些是word2vec想要避免的。

前面说过，word2vec词向量希望体现出一个词语的某种含义，这种“含义”实际上是从该词语的上下文中获得的。word2vec模型是有监督的学习模型。数据集中，除了词语的one-hot编码，还有该词语的上下文环境作为“标签值”。简单地说，标签值可以是输入词语最有可能关联的词语（同样以one-hot编码表示），这样一来，通过不断训练，给定词```x```输出的```N```维向量中最大值所对应的值即为```x```最可能关联的值。当然，实际训练时情况更复杂，往往```y```不是唯一的，毕竟一个词语的上下文环境本身就是复杂的。同时```x```也可能不是唯一的，这种模型通过给定上下文来预测空缺的那个词语。

但是，该模型的训练结果比不被我们所重视，而我们真正需要的其实是模型内部的参数。one-hot编码具有自己的独特性，独特在```1```是唯一的，也即，在训练过程中，当一个one-hot编码被传入时，从输入到隐含层的各个权重中，仅有与```1```相连的权重被激活了，只要one-hot编码不同，那么激活的权重就不同，而前面已经说明，词语与其one-hot编码一一对应，这样一来，经过该模型的训练，所有的词语就会被一一对应于相等数量的权重，而且不难直到，如果隐含层的单元个数为```v```，那么每个词对应的权重数也为```v```，将这```v```个数据放到一个向量中，该向量即为词语的wrod2vec词向量。

由于引入了上下文环境，word2vec词向量具有了表达含义的能力，有训练结果显示，词语king，queen，man，woman的word2vec词向量体现出了```king = queen - woman + man```的奇妙运算关系。

